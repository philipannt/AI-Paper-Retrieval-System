{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0c0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sqlalchemy import (\n",
    "    create_engine, text, MetaData, Table, Column, Integer, String,\n",
    "    Text as SA_Text, DateTime, ForeignKey, UniqueConstraint\n",
    ")\n",
    "from sqlalchemy.dialects.mysql import LONGTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08122ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEI_DIR = r'C:\\Users\\ThienAn\\OneDrive\\Python\\Search-Engine\\data-full'\n",
    "MODEL_DIR = \"models_hybrid_lda_w2v\"\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "env_path = Path('.env')\n",
    "if not env_path.exists():\n",
    "    env_text = (\n",
    "        'MYSQL_HOST=127.0.0.1\\n'\n",
    "        'MYSQL_PORT=3306\\n'\n",
    "        'MYSQL_USER=root\\n'\n",
    "        'MYSQL_PASSWORD=your_password_here\\n'\n",
    "        'MYSQL_DB=ai_papers\\n'\n",
    "        'MYSQL_CHARSET=utf8mb4\\n'\n",
    "        'MYSQL_COLLATION=utf8mb4_unicode_ci\\n'\n",
    "    )\n",
    "    env_path.write_text(env_text)\n",
    "    print('Created .env template. Please update with your MySQL credentials.')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc25059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'ai_papers' ready.\n",
      "Connected to database: ai_papers\n"
     ]
    }
   ],
   "source": [
    "host = os.getenv('MYSQL_HOST', '127.0.0.1')\n",
    "port = os.getenv('MYSQL_PORT', '3306')\n",
    "user = os.getenv('MYSQL_USER', 'root')\n",
    "pwd = os.getenv('MYSQL_PASSWORD', '')\n",
    "db = os.getenv('MYSQL_DB', 'ai_papers')\n",
    "charset = os.getenv('MYSQL_CHARSET', 'utf8mb4')\n",
    "collation = os.getenv('MYSQL_COLLATION', 'utf8mb4_unicode_ci')\n",
    "\n",
    "server_url = f\"mysql+pymysql://{user}:{pwd}@{host}:{port}/?charset={charset}\"\n",
    "server_engine = create_engine(server_url, pool_pre_ping=True)\n",
    "\n",
    "try:\n",
    "    with server_engine.begin() as conn:\n",
    "        conn.execute(text(f\"CREATE DATABASE IF NOT EXISTS `{db}` \"\n",
    "                          f\"DEFAULT CHARACTER SET {charset} \"\n",
    "                          f\"DEFAULT COLLATE {collation}\"))\n",
    "    print(f\"Database '{db}' ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database: {e}\")\n",
    "\n",
    "server_engine.dispose()\n",
    "\n",
    "db_url = f\"mysql+pymysql://{user}:{pwd}@{host}:{port}/{db}?charset={charset}\"\n",
    "engine = create_engine(db_url, pool_pre_ping=True, echo=False, pool_size=10, max_overflow=20)\n",
    "print(f\"Connected to database: {db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5faee711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema created successfully.\n"
     ]
    }
   ],
   "source": [
    "metadata = MetaData()\n",
    "\n",
    "papers = Table('papers', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('md5', String(64), nullable=True),\n",
    "    Column('tei_sha', String(64), nullable=True),\n",
    "    Column('title', SA_Text().with_variant(LONGTEXT(), 'mysql')),\n",
    "    Column('abstract', SA_Text().with_variant(LONGTEXT(), 'mysql')),\n",
    "    Column('venue', String(255), nullable=True),\n",
    "    Column('pub_date', String(32), nullable=True),\n",
    "    Column('language', String(16), nullable=True),\n",
    "    Column('publisher', String(255), nullable=True),\n",
    "    Column('created_at', DateTime, server_default=text('CURRENT_TIMESTAMP'))\n",
    ")\n",
    "\n",
    "persons = Table('persons', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('given', String(255)),\n",
    "    Column('family', String(255)),\n",
    "    Column('full', String(511)),\n",
    "    UniqueConstraint('given', 'family', name='uq_person_name')\n",
    ")\n",
    "\n",
    "paper_person = Table('paper_person', metadata,\n",
    "    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), primary_key=True),\n",
    "    Column('person_id', Integer, ForeignKey('persons.id', ondelete='CASCADE'), primary_key=True),\n",
    "    Column('role', String(32), default='author'),\n",
    "    Column('seq', Integer, default=0)\n",
    ")\n",
    "\n",
    "identifiers = Table('identifiers', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), index=True),\n",
    "    Column('id_type', String(64)),\n",
    "    Column('value', String(512)),\n",
    "    UniqueConstraint('paper_id', 'id_type', 'value', name='uq_identifier')\n",
    ")\n",
    "\n",
    "keywords = Table('keywords', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('term', String(255), unique=True)\n",
    ")\n",
    "\n",
    "paper_keyword = Table('paper_keyword', metadata,\n",
    "    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), primary_key=True),\n",
    "    Column('keyword_id', Integer, ForeignKey('keywords.id', ondelete='CASCADE'), primary_key=True)\n",
    ")\n",
    "\n",
    "sections = Table('sections', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), index=True),\n",
    "    Column('sec_n', String(32), nullable=True),\n",
    "    Column('sec_head', SA_Text().with_variant(LONGTEXT(), 'mysql')),\n",
    "    Column('sec_text', SA_Text().with_variant(LONGTEXT(), 'mysql'))\n",
    ")\n",
    "\n",
    "paper_chunks = Table('paper_chunks', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), index=True),\n",
    "    Column('chunk_index', Integer),\n",
    "    Column('section_head', SA_Text().with_variant(LONGTEXT(), 'mysql')),\n",
    "    Column('content', SA_Text().with_variant(LONGTEXT(), 'mysql'))\n",
    ")\n",
    "\n",
    "DROP_AND_RECREATE = False\n",
    "\n",
    "if DROP_AND_RECREATE:\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 0\"))\n",
    "            for table in ['paper_chunks', 'sections', 'paper_keyword', 'paper_person', \n",
    "                         'identifiers', 'keywords', 'persons', 'papers']:\n",
    "                conn.execute(text(f\"DROP TABLE IF EXISTS {table}\"))\n",
    "            conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 1\"))\n",
    "        print(\"Dropped all existing tables.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping tables: {e}\")\n",
    "else:\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(\"DROP INDEX IF EXISTS ix_papers_title ON papers\"))\n",
    "            conn.execute(text(\"DROP INDEX IF EXISTS ix_paper_chunks_paper_id ON paper_chunks\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    with engine.begin() as conn:\n",
    "        metadata.create_all(conn)\n",
    "    print(\"Schema created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating schema: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbf97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "_ID_ATTR_RE = re.compile(r'(\\s(?:xml:)?id\\s*=\\s*\")([^\"]+)(\")')\n",
    "\n",
    "def _sanitize_xml_ids(xml_text: str) -> str:\n",
    "    \"\"\"Xử lý các ID trùng lặp trong XML\"\"\"\n",
    "    seen = {}\n",
    "    def _repl(m):\n",
    "        head, val, tail = m.groups()\n",
    "        if val not in seen:\n",
    "            seen[val] = 0\n",
    "            return head + val + tail\n",
    "        seen[val] += 1\n",
    "        new_val = f\"{val}__dup{seen[val]}\"\n",
    "        return head + new_val + tail\n",
    "    return _ID_ATTR_RE.sub(_repl, xml_text)\n",
    "\n",
    "def _safe_parse(tei_path: str) -> etree._ElementTree:\n",
    "    \"\"\"Parse XML file một cách an toàn\"\"\"\n",
    "    parser = etree.XMLParser(\n",
    "        recover=True,\n",
    "        huge_tree=True,\n",
    "        resolve_entities=False,\n",
    "        remove_blank_text=False\n",
    "    )\n",
    "    try:\n",
    "        return etree.parse(tei_path, parser)\n",
    "    except etree.XMLSyntaxError:\n",
    "        with open(tei_path, 'rb') as f:\n",
    "            raw = f.read()\n",
    "        txt = raw.decode('utf-8', errors='ignore')\n",
    "        fixed = _sanitize_xml_ids(txt)\n",
    "        return etree.parse(io.BytesIO(fixed.encode('utf-8')), parser)\n",
    "\n",
    "def get_text(node):\n",
    "    \"\"\"Lấy text từ node XML\"\"\"\n",
    "    if node is None:\n",
    "        return None\n",
    "    return \" \".join(\" \".join(node.itertext()).split()) or None\n",
    "\n",
    "def parse_tei(tei_path: str) -> dict:\n",
    "    \"\"\"Parse file TEI XML và trả về dictionary chứa thông tin bài báo\"\"\"\n",
    "    tree = _safe_parse(tei_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Lấy title\n",
    "    title = root.find(\".//tei:teiHeader/tei:fileDesc/tei:titleStmt/tei:title[@type='main']\", NS)\n",
    "    if title is None:\n",
    "        title = root.find(\".//tei:teiHeader/tei:fileDesc/tei:titleStmt/tei:title\", NS)\n",
    "    title = get_text(title)\n",
    "    \n",
    "    # Lấy abstract\n",
    "    abstract_node = root.find(\".//tei:teiHeader/tei:profileDesc/tei:abstract\", NS)\n",
    "    abstract = get_text(abstract_node)\n",
    "    \n",
    "    # Lấy language\n",
    "    lang = root.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "    \n",
    "    # Lấy publisher\n",
    "    publisher_node = root.find(\".//tei:teiHeader/tei:fileDesc/tei:publicationStmt/tei:publisher\", NS)\n",
    "    publisher = get_text(publisher_node)\n",
    "    \n",
    "    # Lấy publication date\n",
    "    date_node = root.find(\".//tei:teiHeader/tei:fileDesc/tei:publicationStmt/tei:date[@type='published']\", NS)\n",
    "    pub_date = get_text(date_node) or (date_node.get(\"when\") if date_node is not None else None)\n",
    "    \n",
    "    # Lấy venue\n",
    "    venue_node = root.find(\".//tei:teiHeader/tei:sourceDesc/tei:biblStruct/tei:monogr/tei:title\", NS)\n",
    "    venue = get_text(venue_node)\n",
    "    \n",
    "    # Lấy identifiers\n",
    "    ids = []\n",
    "    for idno in root.findall(\".//tei:teiHeader//tei:idno\", NS):\n",
    "        id_type = idno.get(\"type\") or \"unknown\"\n",
    "        val = (idno.text or \"\").strip() or None\n",
    "        if val:\n",
    "            ids.append((id_type, val))\n",
    "    \n",
    "    # Lấy MD5\n",
    "    md5 = None\n",
    "    for t, v in ids:\n",
    "        if t.upper() == \"MD5\":\n",
    "            md5 = v\n",
    "    \n",
    "    # Lấy authors\n",
    "    authors = []\n",
    "    for auth in root.findall(\".//tei:teiHeader//tei:sourceDesc//tei:analytic/tei:author\", NS):\n",
    "        given = get_text(auth.find(\".//tei:persName/tei:forename[@type='first']\", NS)) or \\\n",
    "                get_text(auth.find(\".//tei:persName/tei:forename\", NS))\n",
    "        family = get_text(auth.find(\".//tei:persName/tei:surname\", NS))\n",
    "        full = get_text(auth.find(\".//tei:persName\", NS))\n",
    "        full = full or \" \".join(filter(None, [given, family])) or None\n",
    "        authors.append({\"given\": given, \"family\": family, \"full\": full})\n",
    "    \n",
    "    # Lấy keywords\n",
    "    kws = []\n",
    "    for term in root.findall(\".//tei:teiHeader//tei:profileDesc//tei:textClass//tei:keywords//tei:term\", NS):\n",
    "        t = get_text(term)\n",
    "        if t:\n",
    "            for p in re.split(r\"[;,/•]| and \", t):\n",
    "                p = \" \".join(p.split())\n",
    "                if p:\n",
    "                    kws.append(p)\n",
    "    kws = list(dict.fromkeys([k.lower() for k in kws]))\n",
    "    \n",
    "    # Lấy sections\n",
    "    sections_out = []\n",
    "    for head in root.findall(\".//tei:text/tei:body//tei:head\", NS):\n",
    "        sec_head = get_text(head)\n",
    "        sec_n = head.get(\"n\")\n",
    "        texts = []\n",
    "        parent = head.getparent()\n",
    "        started = False\n",
    "        for child in parent:\n",
    "            if child is head:\n",
    "                started = True\n",
    "                continue\n",
    "            if started and child.tag.endswith(\"head\"):\n",
    "                break\n",
    "            if started:\n",
    "                texts.append(get_text(child))\n",
    "        sec_text = \" \".join([t for t in texts if t])\n",
    "        sections_out.append({\"n\": sec_n, \"head\": sec_head, \"text\": sec_text})\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"language\": lang,\n",
    "        \"publisher\": publisher,\n",
    "        \"pub_date\": pub_date,\n",
    "        \"venue\": venue,\n",
    "        \"ids\": ids,\n",
    "        \"md5\": md5,\n",
    "        \"authors\": authors,\n",
    "        \"keywords\": kws,\n",
    "        \"sections\": sections_out,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23710a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def clean_text_block(s, lang='en'):\n",
    "    s = normalize_whitespace(s)\n",
    "    if s and len(s) > 3:\n",
    "        s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def upsert_person(conn, given, family, full):\n",
    "    res = conn.execute(\n",
    "        text('SELECT id FROM persons WHERE given <=> :given AND family <=> :family'),\n",
    "        {'given': given, 'family': family}\n",
    "    ).fetchone()\n",
    "    if res:\n",
    "        return res[0]\n",
    "    try:\n",
    "        res = conn.execute(\n",
    "            text('INSERT INTO persons(given, family, full) VALUES(:g, :f, :full)'),\n",
    "            {'g': given, 'f': family, 'full': full}\n",
    "        )\n",
    "        return res.inserted_primary_key[0]\n",
    "    except Exception:\n",
    "        return conn.execute(\n",
    "            text('SELECT id FROM persons WHERE given <=> :given AND family <=> :family'),\n",
    "            {'given': given, 'family': family}\n",
    "        ).fetchone()[0]\n",
    "\n",
    "def upsert_keyword(conn, term):\n",
    "    res = conn.execute(text('SELECT id FROM keywords WHERE term=:t'), {'t': term}).fetchone()\n",
    "    if res:\n",
    "        return res[0]\n",
    "    try:\n",
    "        res = conn.execute(text('INSERT INTO keywords(term) VALUES(:t)'), {'t': term})\n",
    "        return res.inserted_primary_key[0]\n",
    "    except Exception:\n",
    "        return conn.execute(text('SELECT id FROM keywords WHERE term=:t'), {'t': term}).fetchone()[0]\n",
    "\n",
    "def insert_identifier(conn, paper_id, id_type, value):\n",
    "    try:\n",
    "        conn.execute(\n",
    "            text('INSERT IGNORE INTO identifiers(paper_id, id_type, value) VALUES(:p, :t, :v)'),\n",
    "            {'p': paper_id, 't': id_type, 'v': value}\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def insert_section(conn, paper_id, sec_n, sec_head, sec_text):\n",
    "    conn.execute(\n",
    "        text('INSERT INTO sections(paper_id, sec_n, sec_head, sec_text) VALUES(:p, :n, :h, :x)'),\n",
    "        {'p': paper_id, 'n': sec_n, 'h': sec_head, 'x': sec_text}\n",
    "    )\n",
    "\n",
    "def upsert_paper(conn, rec):\n",
    "    res = conn.execute(\n",
    "        text('SELECT id FROM papers WHERE (md5 IS NOT NULL AND md5 = :md5) OR '\n",
    "             '(title <=> :title AND pub_date <=> :pub_date) ORDER BY id LIMIT 1'),\n",
    "        {'md5': rec.get('md5'), 'title': rec.get('title'), 'pub_date': rec.get('pub_date')}\n",
    "    ).fetchone()\n",
    "    \n",
    "    if res:\n",
    "        paper_id = res[0]\n",
    "        conn.execute(\n",
    "            text('UPDATE papers SET abstract = COALESCE(NULLIF(:abstract, \\'\\'), abstract), '\n",
    "                 'venue = COALESCE(NULLIF(:venue, \\'\\'), venue), '\n",
    "                 'language = COALESCE(NULLIF(:lang, \\'\\'), language), '\n",
    "                 'publisher = COALESCE(NULLIF(:publisher, \\'\\'), publisher) WHERE id=:id'),\n",
    "            dict(abstract=rec.get('abstract') or '', venue=rec.get('venue') or '',\n",
    "                 lang=rec.get('language') or '', publisher=rec.get('publisher') or '', id=paper_id)\n",
    "        )\n",
    "        return paper_id, False\n",
    "    else:\n",
    "        res = conn.execute(\n",
    "            text('INSERT INTO papers(md5, tei_sha, title, abstract, venue, pub_date, language, publisher) '\n",
    "                 'VALUES(:md5, NULL, :title, :abstract, :venue, :pub_date, :language, :publisher)'),\n",
    "            rec\n",
    "        )\n",
    "        try:\n",
    "            return res.inserted_primary_key[0], True\n",
    "        except Exception:\n",
    "            return conn.execute(\n",
    "                text('SELECT id FROM papers WHERE title <=> :t ORDER BY id DESC LIMIT 1'),\n",
    "                {'t': rec.get('title')}\n",
    "            ).fetchone()[0], True\n",
    "\n",
    "def ingest_tei_file(conn, path: str, verbose=False):\n",
    "    data = parse_tei(path)\n",
    "    data['title'] = clean_text_block(data['title'])\n",
    "    data['abstract'] = clean_text_block(data['abstract'])\n",
    "    \n",
    "    if not data['title']:\n",
    "        data['title'] = Path(path).stem\n",
    "    \n",
    "    pid, is_new = upsert_paper(conn, data)\n",
    "    \n",
    "    for id_type, value in data['ids']:\n",
    "        insert_identifier(conn, pid, id_type, value)\n",
    "    \n",
    "    conn.execute(text('DELETE FROM paper_person WHERE paper_id=:pid'), {'pid': pid})\n",
    "    for i, au in enumerate(data['authors']):\n",
    "        person_id = upsert_person(conn, au.get('given'), au.get('family'), au.get('full'))\n",
    "        try:\n",
    "            conn.execute(\n",
    "                text('INSERT IGNORE INTO paper_person(paper_id, person_id, role, seq) VALUES(:p, :r, \\'author\\', :s)'),\n",
    "                {'p': pid, 'r': person_id, 's': i}\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    conn.execute(text('DELETE FROM paper_keyword WHERE paper_id=:pid'), {'pid': pid})\n",
    "    for kw in data['keywords']:\n",
    "        kid = upsert_keyword(conn, kw)\n",
    "        try:\n",
    "            conn.execute(\n",
    "                text('INSERT INTO paper_keyword(paper_id, keyword_id) VALUES(:p, :k)'),\n",
    "                {'p': pid, 'k': kid}\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    conn.execute(text('DELETE FROM sections WHERE paper_id=:pid'), {'pid': pid})\n",
    "    for sec in data['sections']:\n",
    "        insert_section(\n",
    "            conn, pid, sec.get('n'),\n",
    "            clean_text_block(sec.get('head')),\n",
    "            clean_text_block(sec.get('text'))\n",
    "        )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[OK] {Path(path).name} → paper_id={pid} ({'new' if is_new else 'updated'})\")\n",
    "    \n",
    "    return pid, is_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09726c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] A-XAI-BASED-FRAMEWORK-FOR-FREQUENCY-SUBBAND-CHARACTERIZATION-OF-COUGH-SPECTROGRAMS-IN-CHRONIC-RESPIRATORY-DISEASE-A-PREPRINT.tei.xml → paper_id=100 (new)\n",
      "[OK] AN-EFFICIENT-CONDITIONAL-SCORE-BASED-FILTER-FOR-HIGH-DIMENSIONAL-NONLINEAR-FILTERING-PROBLEMS.tei.xml → paper_id=200 (new)\n",
      "[OK] Autiverse-Eliciting-Autistic-Adolescents'-Daily-Narratives-through-AI-guided-Multimodal-Journaling.tei.xml → paper_id=300 (new)\n",
      "[OK] BLIP-2-Bootstrapping-Language-Image-Pre-training-with-Frozen-Image-Encoders-and-Large-Language-Models.tei.xml → paper_id=400 (new)\n",
      "[OK] Classification-errors-distort-findings-in-automated-speech-processing-examples-and-solutions-from-child-development-research.tei.xml → paper_id=500 (new)\n",
      "[OK] CPCLDETECTOR-KNOWLEDGE-ENHANCEMENT-AND-ALIGNMENT-SELECTION-FOR-CHINESE-PATRONIZING-AND-CONDESCENDING-LANGUAGE-DETECTION.tei.xml → paper_id=600 (new)\n",
      "[OK] Designing-Network-Design-Spaces.tei.xml → paper_id=700 (new)\n",
      "[OK] Drag-Your-GAN-Interactive-Point-based-Manipulation-on-the-Generative-Image-Manifold.tei.xml → paper_id=800 (new)\n",
      "[OK] Empowering-Healthcare-Practitioners-with-Language-Models-Structuring-Speech-Transcripts-in-Two-Real-World-Clinical-Applications.tei.xml → paper_id=899 (new)\n",
      "[OK] Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer.tei.xml → paper_id=999 (new)\n",
      "[OK] FedHiP-Heterogeneity-Invariant-Personalized-Federated-Learning-Through-Closed-Form-Solutions.tei.xml → paper_id=1099 (new)\n",
      "[OK] FOCALCODEC-STREAM-STREAMING-LOW-BITRATE-SPEECH-CODING-VIA-CAUSAL-DISTILLATION.tei.xml → paper_id=1199 (new)\n",
      "[OK] GLiNER-Generalist-Model-for-Named-Entity-Recognition-using-Bidirectional-Transformer.tei.xml → paper_id=1299 (new)\n",
      "[OK] Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets.tei.xml → paper_id=1399 (new)\n",
      "[OK] Interactive-Recommendation-Agent-with-Active-User-Commands.tei.xml → paper_id=1498 (new)\n",
      "[OK] Large-Language-Models-Understand-and-Can-Be-Enhanced-by-Emotional-Stimuli.tei.xml → paper_id=1598 (new)\n",
      "[OK] LLaMA-Adapter-V2-Parameter-Efficient-Visual-Instruction-Model.tei.xml → paper_id=1697 (new)\n",
      "[OK] MatFormer-Nested-Transformer-for-Elastic-Inference.tei.xml → paper_id=1796 (new)\n",
      "[OK] MO-R-CNN-Multispectral-Oriented-R-CNN-for-Object-Detection-in-Remote-Sensing-Image.tei.xml → paper_id=1896 (new)\n",
      "[OK] NEAT-Concept-driven-Neuron-Attribution-in-LLMs.tei.xml → paper_id=1995 (new)\n",
      "[OK] Online-Anti-sexist-Speech-Identifying-Resistance-to-Gender-Bias-in-Political-Discourse.tei.xml → paper_id=2095 (new)\n",
      "[OK] PipelineRL-Faster-On-policy-Reinforcement-Learning-for-Long-Sequence-Generation.tei.xml → paper_id=2195 (new)\n",
      "[OK] Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learning.tei.xml → paper_id=2295 (new)\n",
      "[OK] ReTrack-Data-Unlearning-in-Diffusion-Models-through-Redirecting-the-Denoising-Trajectory.tei.xml → paper_id=2395 (new)\n",
      "[OK] SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines.tei.xml → paper_id=2495 (new)\n",
      "[OK] SISMA-Semantic-Face-Image-Synthesis-with-Mamba.tei.xml → paper_id=2595 (new)\n",
      "[OK] Stage-Diff-Stage-wise-Long-Term-Time-Series-Generation-Based-on-Diffusion-Models.tei.xml → paper_id=2695 (new)\n",
      "[OK] Test-Time-Warmup-for-Multimodal-Large-Language-Models.tei.xml → paper_id=2795 (new)\n",
      "[OK] TORA-A-TOOL-INTEGRATED-REASONING-AGENT-FOR-MATHEMATICAL-PROBLEM-SOLVING.tei.xml → paper_id=2895 (new)\n",
      "[OK] Uncovering-Graph-Reasoning-in-Decoder-only-Transformers-with-Circuit-Tracing.tei.xml → paper_id=2995 (new)\n",
      "[OK] Vision-Free-Retrieval-Rethinking-Multimodal-Search-with-Textual-Scene-Descriptions.tei.xml → paper_id=3095 (new)\n",
      "[OK] X-Troll-eXplainable-Detection-of-State-Sponsored-Information-Operations-Agents.tei.xml → paper_id=3195 (new)\n",
      "Imported 3232 files: 3222 new, 4 updated in 76.5s\n",
      "6 files failed. Sample errors:\n",
      "  A-Simple-Approach-to-Enhance-Text-to-Image-Models-via-Chain-of-Thought-Prompt-Rewriting.tei.xml: TypeError(\"'NoneType' object is not subscriptable\")\n",
      "  A-Survey-of-the-State-of-the-Art-in-Conversational-Question-Answering-Systems.tei.xml: TypeError(\"'NoneType' object is not subscriptable\")\n",
      "  Efficient-Methods-for-Natural-Language-Processing-A-Survey.tei.xml: DataError('(pymysql.err.DataError) (1406, \"Data too long for column \\'pub_date\\' at row 1\")')\n",
      "  FOLLOWIR-Evaluating-and-Teaching-Information-Retrieval-Models-to-Follow-Instructions.tei.xml: TypeError(\"'NoneType' object is not subscriptable\")\n",
      "  LLM-Analysis-of-150+-years-of-German-Parliamentary-Debates-on-Migration-Reveals-Shift-from-Post-War-Solidarity-to-Anti-Solidarity-in-the-Last-Decade.tei.xml: TypeError(\"'NoneType' object is not subscriptable\")\n"
     ]
    }
   ],
   "source": [
    "def ingest_folder(engine, tei_dir: str, pattern=\"*.tei.xml\", verbose_every=100):\n",
    "    tei_dir = Path(tei_dir)\n",
    "    files = sorted(tei_dir.rglob(pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No TEI files found in {tei_dir.resolve()}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    new, updated = 0, 0\n",
    "    errors = []\n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        for idx, f in enumerate(files, 1):\n",
    "            try:\n",
    "                pid, is_new = ingest_tei_file(conn, str(f), verbose=(verbose_every and idx % verbose_every == 0))\n",
    "                if is_new:\n",
    "                    new += 1\n",
    "                else:\n",
    "                    updated += 1\n",
    "            except Exception as e:\n",
    "                errors.append((str(f), repr(e)))\n",
    "                if verbose_every and idx % verbose_every == 0:\n",
    "                    print(f\"[ERROR] {f.name} :: {repr(e)}\")\n",
    "    \n",
    "    dt = (datetime.now() - t0).total_seconds()\n",
    "    print(f\"Imported {len(files)} files: {new} new, {updated} updated in {dt:.1f}s\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"{len(errors)} files failed. Sample errors:\")\n",
    "        for fname, err in errors[:5]:\n",
    "            print(f\"  {Path(fname).name}: {err}\")\n",
    "    \n",
    "    return new, updated\n",
    "\n",
    "new, updated = ingest_folder(engine, TEI_DIR, verbose_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520ea92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 103994/103994 [00:01<00:00, 65631.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 157484 chunks from 103994 sections\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT p.id AS paper_id, p.title, s.sec_head, s.sec_text\n",
    "FROM sections s\n",
    "JOIN papers p ON p.id = s.paper_id\n",
    "WHERE s.sec_text IS NOT NULL AND LENGTH(s.sec_text) > 30\n",
    "\"\"\"\n",
    "\n",
    "sec = pd.read_sql(sql, engine)\n",
    "sec[\"sec_head\"] = sec[\"sec_head\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "sec[\"sec_text\"] = sec[\"sec_text\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "def split_to_chunks(text, max_len=1200, min_len=600, sep=r\"(?<=[\\.\\?\\!])\\s+\"):\n",
    "    if not text:\n",
    "        return []\n",
    "    sents = re.split(sep, text)\n",
    "    chunks, cur = [], \"\"\n",
    "    for s in sents:\n",
    "        if not s:\n",
    "            continue\n",
    "        if len(cur) + len(s) + 1 <= max_len:\n",
    "            cur = (cur + \" \" + s).strip()\n",
    "        else:\n",
    "            if len(cur) < min_len and chunks:\n",
    "                chunks[-1] = (chunks[-1] + \" \" + cur + \" \" + s).strip()\n",
    "                cur = \"\"\n",
    "            else:\n",
    "                if cur:\n",
    "                    chunks.append(cur)\n",
    "                cur = s\n",
    "    if cur:\n",
    "        if len(cur) < min_len and chunks:\n",
    "            chunks[-1] = (chunks[-1] + \" \" + cur).strip()\n",
    "        else:\n",
    "            chunks.append(cur)\n",
    "    return chunks\n",
    "\n",
    "rows = []\n",
    "for r in tqdm(sec.itertuples(index=False), total=len(sec), desc=\"Chunking\"):\n",
    "    chunks = split_to_chunks(r.sec_text, max_len=1200, min_len=600)\n",
    "    if not chunks:\n",
    "        chunks = [r.sec_text] if r.sec_text else []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        rows.append({\n",
    "            \"paper_id\": r.paper_id,\n",
    "            \"title\": r.title,\n",
    "            \"sec_head\": r.sec_head,\n",
    "            \"chunk_idx\": i,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "chdf = pd.DataFrame(rows)\n",
    "chdf = chdf[chdf[\"text\"].str.len() > 30].reset_index(drop=True)\n",
    "\n",
    "print(f\"Created {len(chdf)} chunks from {len(sec)} sections\")\n",
    "chdf.to_csv(os.path.join(MODEL_DIR, \"chunks_meta.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7dc3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 157484 chunks to database\n"
     ]
    }
   ],
   "source": [
    "upload_df = chdf[['paper_id', 'chunk_idx', 'sec_head', 'text']].copy()\n",
    "upload_df.columns = ['paper_id', 'chunk_index', 'section_head', 'content']\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DELETE FROM paper_chunks\"))\n",
    "\n",
    "upload_df.to_sql(\n",
    "    'paper_chunks',\n",
    "    con=engine,\n",
    "    if_exists='append',\n",
    "    index=False,\n",
    "    method='multi',\n",
    "    chunksize=5000\n",
    ")\n",
    "\n",
    "print(f\"Uploaded {len(upload_df)} chunks to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3c6986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 157484/157484 [00:03<00:00, 48390.06it/s]\n"
     ]
    }
   ],
   "source": [
    "TOKEN_RE = re.compile(r\"[a-zA-Z][a-zA-Z0-9\\-]+\")\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    if isinstance(text, (list, tuple)):\n",
    "        text = \" \".join(map(str, text))\n",
    "    try:\n",
    "        text = text.lower()\n",
    "    except Exception:\n",
    "        text = str(text).lower()\n",
    "    toks = TOKEN_RE.findall(text)\n",
    "    return [t for t in toks if 2 <= len(t) <= 40]\n",
    "\n",
    "tokens_list = [tokenize(t) for t in tqdm(chdf[\"text\"].tolist(), desc=\"Tokenizing\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e14518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved: vocab_size=109144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating W2V vectors: 100%|██████████| 157484/157484 [00:12<00:00, 12880.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V vectors saved: (157484, 200)\n"
     ]
    }
   ],
   "source": [
    "W2V_DIM = 200\n",
    "W2V_WINDOW = 5\n",
    "W2V_MIN_COUNT = 2\n",
    "W2V_WORKERS = 4\n",
    "\n",
    "w2v_chunks = Word2Vec(\n",
    "    sentences=tokens_list,\n",
    "    vector_size=W2V_DIM,\n",
    "    window=W2V_WINDOW,\n",
    "    min_count=W2V_MIN_COUNT,\n",
    "    workers=W2V_WORKERS,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "w2v_model_path = os.path.join(MODEL_DIR, \"word2vec_chunks.model\")\n",
    "w2v_chunks.save(w2v_model_path)\n",
    "print(f\"Word2Vec model saved: vocab_size={len(w2v_chunks.wv.key_to_index)}\")\n",
    "\n",
    "def docvec_w2v(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv.key_to_index]\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    v = np.mean(vecs, axis=0).astype(np.float32)\n",
    "    n = np.linalg.norm(v) + 1e-12\n",
    "    return v / n\n",
    "\n",
    "chunkvecs_w2v = np.vstack([\n",
    "    docvec_w2v(toks, w2v_chunks) \n",
    "    for toks in tqdm(tokens_list, desc=\"Creating W2V vectors\")\n",
    "])\n",
    "\n",
    "w2v_vectors_path = os.path.join(MODEL_DIR, \"chunkvecs_w2v.npy\")\n",
    "np.save(w2v_vectors_path, chunkvecs_w2v)\n",
    "print(f\"W2V vectors saved: {chunkvecs_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e937df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 73227, Corpus size: 157484\n",
      "LDA model saved: 100 topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LDA vectors: 100%|██████████| 157484/157484 [00:37<00:00, 4191.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA vectors saved: (157484, 100)\n"
     ]
    }
   ],
   "source": [
    "LDA_TOPICS = 100\n",
    "LDA_PASSES = 5\n",
    "LDA_ALPHA = 'symmetric'\n",
    "\n",
    "dictionary_chunks = Dictionary(tokens_list)\n",
    "dictionary_chunks.filter_extremes(no_below=3, no_above=0.5, keep_n=100000)\n",
    "corpus_bow_chunks = [dictionary_chunks.doc2bow(toks) for toks in tokens_list]\n",
    "\n",
    "print(f\"Dictionary size: {len(dictionary_chunks)}, Corpus size: {len(corpus_bow_chunks)}\")\n",
    "\n",
    "lda_chunks = LdaModel(\n",
    "    corpus=corpus_bow_chunks,\n",
    "    id2word=dictionary_chunks,\n",
    "    num_topics=LDA_TOPICS,\n",
    "    passes=LDA_PASSES,\n",
    "    alpha=LDA_ALPHA,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dict_path = os.path.join(MODEL_DIR, \"lda_dictionary_chunks.pkl\")\n",
    "with open(dict_path, \"wb\") as f:\n",
    "    pickle.dump(dictionary_chunks, f)\n",
    "\n",
    "lda_model_path = os.path.join(MODEL_DIR, \"lda_model_chunks.gensim\")\n",
    "lda_chunks.save(lda_model_path)\n",
    "print(f\"LDA model saved: {LDA_TOPICS} topics\")\n",
    "\n",
    "def dense_topic_vec(bow, lda_model, K):\n",
    "    dist = np.zeros(K, dtype=np.float32)\n",
    "    for tid, prob in lda_model.get_document_topics(bow, minimum_probability=0.0):\n",
    "        if 0 <= tid < K:\n",
    "            dist[tid] = prob\n",
    "    n = np.linalg.norm(dist) + 1e-12\n",
    "    return dist / n\n",
    "\n",
    "chunkvecs_lda = np.vstack([\n",
    "    dense_topic_vec(bow, lda_chunks, LDA_TOPICS) \n",
    "    for bow in tqdm(corpus_bow_chunks, desc=\"Creating LDA vectors\")\n",
    "])\n",
    "\n",
    "lda_vectors_path = os.path.join(MODEL_DIR, \"chunkvecs_lda.npy\")\n",
    "np.save(lda_vectors_path, chunkvecs_lda)\n",
    "print(f\"LDA vectors saved: {chunkvecs_lda.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2268b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_HYBRID = 0.5\n",
    "\n",
    "def query_to_w2v_vec(q, model):\n",
    "    toks = tokenize(q)\n",
    "    return docvec_w2v(toks, model).reshape(1, -1)\n",
    "\n",
    "def query_to_lda_vec(q, dictionary, lda_model, K):\n",
    "    toks = tokenize(q)\n",
    "    bow = dictionary.doc2bow(toks)\n",
    "    return dense_topic_vec(bow, lda_model, K).reshape(1, -1)\n",
    "\n",
    "def minmax(x):\n",
    "    mn, mx = float(np.min(x)), float(np.max(x))\n",
    "    if mx - mn < 1e-12:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "def search_w2v_chunks(query, topk=10):\n",
    "    \"\"\"Tìm kiếm chỉ sử dụng mô hình Word2Vec\"\"\"\n",
    "    q_w2v = query_to_w2v_vec(query, w2v_chunks)\n",
    "    s_w2v = cosine_similarity(q_w2v, chunkvecs_w2v)[0]\n",
    "    s_w2v = minmax(s_w2v)\n",
    "    \n",
    "    idx = np.argsort(-s_w2v)[:topk]\n",
    "    out = chdf.iloc[idx].copy()\n",
    "    out[\"score\"] = s_w2v[idx]\n",
    "    out[\"preview\"] = out[\"text\"].str[:500]\n",
    "    \n",
    "    return out.reset_index(drop=True)[\n",
    "        [\"paper_id\", \"title\", \"sec_head\", \"chunk_idx\", \"score\", \"preview\"]\n",
    "    ]\n",
    "\n",
    "def search_lda_chunks(query, topk=10):\n",
    "    \"\"\"Tìm kiếm chỉ sử dụng mô hình LDA\"\"\"\n",
    "    q_lda = query_to_lda_vec(query, dictionary_chunks, lda_chunks, LDA_TOPICS)\n",
    "    s_lda = cosine_similarity(q_lda, chunkvecs_lda)[0]\n",
    "    s_lda = minmax(s_lda)\n",
    "    \n",
    "    idx = np.argsort(-s_lda)[:topk]\n",
    "    out = chdf.iloc[idx].copy()\n",
    "    out[\"score\"] = s_lda[idx]\n",
    "    out[\"preview\"] = out[\"text\"].str[:500]\n",
    "    \n",
    "    return out.reset_index(drop=True)[\n",
    "        [\"paper_id\", \"title\", \"sec_head\", \"chunk_idx\", \"score\", \"preview\"]\n",
    "    ]\n",
    "\n",
    "def hybrid_search_chunks(query, topk=10, alpha=ALPHA_HYBRID):\n",
    "    \"\"\"Tìm kiếm kết hợp cả Word2Vec và LDA\"\"\"\n",
    "    q_w2v = query_to_w2v_vec(query, w2v_chunks)\n",
    "    q_lda = query_to_lda_vec(query, dictionary_chunks, lda_chunks, LDA_TOPICS)\n",
    "    \n",
    "    s_w2v = cosine_similarity(q_w2v, chunkvecs_w2v)[0]\n",
    "    s_lda = cosine_similarity(q_lda, chunkvecs_lda)[0]\n",
    "    \n",
    "    s_w2v = minmax(s_w2v)\n",
    "    s_lda = minmax(s_lda)\n",
    "    score = alpha * s_w2v + (1 - alpha) * s_lda\n",
    "    \n",
    "    idx = np.argsort(-score)[:topk]\n",
    "    out = chdf.iloc[idx].copy()\n",
    "    out[\"score\"] = score[idx]\n",
    "    out[\"score_w2v\"] = s_w2v[idx]\n",
    "    out[\"score_lda\"] = s_lda[idx]\n",
    "    out[\"preview\"] = out[\"text\"].str[:500]\n",
    "    \n",
    "    return out.reset_index(drop=True)[\n",
    "        [\"paper_id\", \"title\", \"sec_head\", \"chunk_idx\", \"score\", \"score_w2v\", \"score_lda\", \"preview\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a70f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'neural networks and deep learning'\n",
      "Results: 10 chunks\n",
      "\n",
      "Score range: 0.8272 - 0.8940\n",
      "Mean score: 0.8547\n",
      "W2V score range: 0.8052 - 1.0000\n",
      "LDA score range: 0.7524 - 0.9227\n",
      "\n",
      "[1] Score: 0.8940 | Paper ID: 2274\n",
      "    Title: Published as a conference paper at ICLR 2015 ADAM: A METHOD FOR STOCHASTIC OPTIM...\n",
      "    Section: 6. 3 Figure 2 :Figure 3 :\n",
      "    Preview: 323 Figure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using dropout stochastic regularization. (b) Neural networks...\n",
      "\n",
      "[2] Score: 0.8809 | Paper ID: 2910\n",
      "    Title: Towards End-to-End Speech Recognition with Recurrent Neural Networks...\n",
      "    Section: Figure 3 .\n",
      "    Preview: 3 Figure 3. Deep Recurrent Neural Network....\n",
      "\n",
      "[3] Score: 0.8780 | Paper ID: 1403\n",
      "    Title: HYBRID SPEECH RECOGNITION WITH DEEP BIDIRECTIONAL LSTM...\n",
      "    Section: Fig. 3 .Fig. 4 .\n",
      "    Preview: 34 Fig. 3. Deep Recurrent Neural Network...\n",
      "\n",
      "[4] Score: 0.8639 | Paper ID: 2053\n",
      "    Title: Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations...\n",
      "    Section: Related Work\n",
      "    Preview: We provide a unifying framework that connects metric learning to (optimal) offline goal-conditioned reinforcement learning (GCRL)....\n",
      "\n",
      "[5] Score: 0.8538 | Paper ID: 2188\n",
      "    Title: Physics-Informed Neural Network Approaches for Sparse Data Flow Reconstruction o...\n",
      "    Section: 3 .\n",
      "    Preview: 3 Learning bias, where the governing Partial Differential Equation (PDE) of the physical phenomenon is enforced as a penalty constraint in the loss fu...\n",
      "\n",
      "[6] Score: 0.8470 | Paper ID: 1636\n",
      "    Title: learning-features-2009-TR.tei...\n",
      "    Section: Figure 1 . 6 :\n",
      "    Preview: 16 Figure 1.6: The Restricted Boltzmann Machine architecture....\n",
      "\n",
      "[7] Score: 0.8408 | Paper ID: 1641\n",
      "    Title: Learning Robust Penetration-Testing Policies under Partial Observability: A syst...\n",
      "    Section: Figure 1 :\n",
      "    Preview: 1 Figure1: The agent-environment interaction in reinforcement learning. Adapted from [47] ....\n",
      "\n",
      "[8] Score: 0.8331 | Paper ID: 87\n",
      "    Title: A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks...\n",
      "    Section: Fig. 7 :\n",
      "    Preview: 7 Fig. 7: Taxonomy of intelligent collaboration learning paradigms, including federated learning (FL), distributed deep learning (DDL), edge-cloud mod...\n",
      "\n",
      "[9] Score: 0.8280 | Paper ID: 2083\n",
      "    Title: On the Opportunities and Risks of Foundation Models...\n",
      "    Section: Emergence and homogenization\n",
      "    Preview: At the same time, machine learning homogenizes learning algorithms (e.g., logistic regression), deep learning homogenizes model architectures (e.g., C...\n",
      "\n",
      "[10] Score: 0.8272 | Paper ID: 44\n",
      "    Title: A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip-Sync Synt...\n",
      "    Section: Literature Review\n",
      "    Preview: Advanced voice cloning and talking-head synthesis attained a breakthrough mainly due to deep learning and transformers, as well as generative adversar...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"neural networks and deep learning\"\n",
    "results = hybrid_search_chunks(query, topk=10, alpha=0.5)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Results: {len(results)} chunks\\n\")\n",
    "print(f\"Score range: {results['score'].min():.4f} - {results['score'].max():.4f}\")\n",
    "print(f\"Mean score: {results['score'].mean():.4f}\")\n",
    "print(f\"W2V score range: {results['score_w2v'].min():.4f} - {results['score_w2v'].max():.4f}\")\n",
    "print(f\"LDA score range: {results['score_lda'].min():.4f} - {results['score_lda'].max():.4f}\\n\")\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"[{idx+1}] Score: {row['score']:.4f} | Paper ID: {row['paper_id']}\")\n",
    "    print(f\"    Title: {row['title'][:80]}...\")\n",
    "    print(f\"    Section: {row['sec_head']}\")\n",
    "    print(f\"    Preview: {row['preview'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b0a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_dir=MODEL_DIR):\n",
    "    chdf_loaded = pd.read_csv(os.path.join(model_dir, \"chunks_meta.csv\"))\n",
    "    w2v_loaded = Word2Vec.load(os.path.join(model_dir, \"word2vec_chunks.model\"))\n",
    "    lda_loaded = LdaModel.load(os.path.join(model_dir, \"lda_model_chunks.gensim\"))\n",
    "    with open(os.path.join(model_dir, \"lda_dictionary_chunks.pkl\"), \"rb\") as f:\n",
    "        dict_loaded = pickle.load(f)\n",
    "    chunkvecs_w2v_loaded = np.load(os.path.join(model_dir, \"chunkvecs_w2v.npy\"))\n",
    "    chunkvecs_lda_loaded = np.load(os.path.join(model_dir, \"chunkvecs_lda.npy\"))\n",
    "    \n",
    "    return {\n",
    "        'chunks_meta': chdf_loaded,\n",
    "        'w2v_model': w2v_loaded,\n",
    "        'lda_model': lda_loaded,\n",
    "        'dictionary': dict_loaded,\n",
    "        'w2v_vectors': chunkvecs_w2v_loaded,\n",
    "        'lda_vectors': chunkvecs_lda_loaded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959c99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded: 157484 chunks, W2V vocab=109144, LDA topics=100\n"
     ]
    }
   ],
   "source": [
    "chdf_loaded = pd.read_csv(os.path.join(MODEL_DIR, \"chunks_meta.csv\"))\n",
    "w2v_loaded = Word2Vec.load(os.path.join(MODEL_DIR, \"word2vec_chunks.model\"))\n",
    "lda_loaded = LdaModel.load(os.path.join(MODEL_DIR, \"lda_model_chunks.gensim\"))\n",
    "with open(os.path.join(MODEL_DIR, \"lda_dictionary_chunks.pkl\"), \"rb\") as f:\n",
    "    dict_loaded = pickle.load(f)\n",
    "chunkvecs_w2v_loaded = np.load(os.path.join(MODEL_DIR, \"chunkvecs_w2v.npy\"))\n",
    "chunkvecs_lda_loaded = np.load(os.path.join(MODEL_DIR, \"chunkvecs_lda.npy\"))\n",
    "\n",
    "chdf = chdf_loaded\n",
    "w2v_chunks = w2v_loaded\n",
    "lda_chunks = lda_loaded\n",
    "dictionary_chunks = dict_loaded\n",
    "chunkvecs_w2v = chunkvecs_w2v_loaded\n",
    "chunkvecs_lda = chunkvecs_lda_loaded\n",
    "LDA_TOPICS = lda_loaded.num_topics\n",
    "\n",
    "print(f\"Models loaded: {len(chdf)} chunks, W2V vocab={len(w2v_chunks.wv.key_to_index)}, LDA topics={LDA_TOPICS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c92b536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'transformer architecture attention mechanism'\n",
      "Results: 10 chunks\n",
      "\n",
      "Score range: 0.8653 - 0.9522\n",
      "Mean score: 0.9086\n",
      "\n",
      "[1] Score: 0.9522 | Paper ID: 283\n",
      "    Title: ATTENTIVE CONTEXTUAL CARRYOVER FOR MULTI-TURN END-TO-END SPOKEN LANGUAGE UNDERST...\n",
      "    Section: Fig. 4 .\n",
      "    Preview: 4 Fig. 4. Architecture of Gated Multi-head attentions....\n",
      "\n",
      "[2] Score: 0.9515 | Paper ID: 781\n",
      "    Title: Do Vision Transformers See Like Convolutional Neural Networks?...\n",
      "    Section: Discussion\n",
      "    Preview: Attention 1 MLP 1 Attention 2 MLP 2 Attention 3 MLP Attention 4 MLP 4 Attention 5 MLP 5 Attention 6 MLP Attention 7 MLP 7 Attention 8 MLP 8 Attention ...\n",
      "\n",
      "[3] Score: 0.9378 | Paper ID: 283\n",
      "    Title: ATTENTIVE CONTEXTUAL CARRYOVER FOR MULTI-TURN END-TO-END SPOKEN LANGUAGE UNDERST...\n",
      "    Section: IVA Dataset Syn-Multi Dataset Statistic RNN-T SLU T-T SLU RNN-T SLU T-T SLU Audio encoder network\n",
      "    Preview: # Layers 5 6 4 2 Layer embed-size 736 256 640 256 # Attention heads -...\n",
      "\n",
      "[4] Score: 0.9220 | Paper ID: 279\n",
      "    Title: ATTENTION-BASED MODELS FOR TEXT-DEPENDENT SPEAKER VERIFICATION...\n",
      "    Section: Fig. 3 :\n",
      "    Preview: 3 Fig. 3: Two variants of the attention layer: (a) cross-layer attention; (b) divided-layer attention....\n",
      "\n",
      "[5] Score: 0.9169 | Paper ID: 281\n",
      "    Title: Attention Is All You Need...\n",
      "    Section: ScaledFigure 2 :\n",
      "    Preview: 2 Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel....\n",
      "\n",
      "[6] Score: 0.8946 | Paper ID: 530\n",
      "    Title: COLT5: Faster Long-Range Transformers with Conditional Computation...\n",
      "    Section: Encoder Layer Component Flops\n",
      "    Preview: Vanilla self-attention computation 2n 2 d Attention QKV and output projections 4nd 2 Feedforward layer 8nd 2 LONGT5 local attention computation 2nwd L...\n",
      "\n",
      "[7] Score: 0.8888 | Paper ID: 906\n",
      "    Title: End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection...\n",
      "    Section: Figure 3 :\n",
      "    Preview: 3 Figure3: The architecture of our proposed SPFM, which consists of two scale-aware pyramid-like layers, self attention and multi-head cross attention...\n",
      "\n",
      "[8] Score: 0.8833 | Paper ID: 1140\n",
      "    Title: FG-ATTN: LEVERAGING FINE-GRAINED SPARSITY IN DIFFUSION TRANSFORMERS...\n",
      "    Section: Skipping slices of 64x1 query-key dot products\n",
      "    Preview: Figure 6 . Block sparse attention mechanisms skip tiles of 128 × 128 attention map scores. We propose a method to skip fine-grain 128 × 1 sections of ...\n",
      "\n",
      "[9] Score: 0.8742 | Paper ID: 360\n",
      "    Title: Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models...\n",
      "    Section: Dropout\n",
      "    Preview: GoT input G 𝑁, 𝐸 Graph Attention Layer Graph Attention Layer Concatenate Dropout Graph Attention Layer FFNN Layernorm GoT representation Multi-head at...\n",
      "\n",
      "[10] Score: 0.8653 | Paper ID: 1365\n",
      "    Title: Hierarchical Neural Story Generation...\n",
      "    Section: Figure 2 :\n",
      "    Preview: 2 Figure2: Self-Attention Mechanism of a single head, with GLU gating and downsampling. Multiple heads are concatenated, with each head using a separa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"transformer architecture attention mechanism\"\n",
    "results = hybrid_search_chunks(query, topk=10, alpha=0.5)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Results: {len(results)} chunks\\n\")\n",
    "print(f\"Score range: {results['score'].min():.4f} - {results['score'].max():.4f}\")\n",
    "print(f\"Mean score: {results['score'].mean():.4f}\\n\")\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"[{idx+1}] Score: {row['score']:.4f} | Paper ID: {row['paper_id']}\")\n",
    "    print(f\"    Title: {row['title'][:80]}...\")\n",
    "    print(f\"    Section: {row['sec_head']}\")\n",
    "    print(f\"    Preview: {row['preview'][:150]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
